"""
–ú–æ–¥—É–ª—å –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ LLM
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏
"""

import aiohttp
from aiohttp_socks import ProxyConnector
import logging
from typing import List, Dict, Optional
from datetime import datetime
import json

logger = logging.getLogger(__name__)


class WebSearchTool:
    """–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–µ–±-–ø–æ–∏—Å–∫–∞ –¥–ª—è LLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏"""
    
    def __init__(self, proxy_url: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–±-–ø–æ–∏—Å–∫–∞
        
        Args:
            proxy_url: URL –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        """
        self.session: Optional[aiohttp.ClientSession] = None
        self.proxy_url = proxy_url
        self.search_api = "https://api.duckduckgo.com/"
        
    async def _get_session(self) -> aiohttp.ClientSession:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –ø—Ä–æ–∫—Å–∏"""
        if self.session is None or self.session.closed:
            # –°–æ–∑–¥–∞—ë–º —Å–µ—Å—Å–∏—é —Å –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if self.proxy_url:
                try:
                    connector = ProxyConnector.from_url(self.proxy_url)
                    self.session = aiohttp.ClientSession(connector=connector)
                    logger.info(f"WebSearchTool: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏ {self.proxy_url}")
                except Exception as e:
                    logger.warning(f"WebSearchTool: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ–∫—Å–∏ –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä: {e}")
                    self.session = aiohttp.ClientSession()
            else:
                self.session = aiohttp.ClientSession()
        return self.session
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        if self.session and not self.session.closed:
            await self.session.close()
    
    async def search_duckduckgo(
        self,
        query: str,
        max_results: int = 5
    ) -> List[Dict[str, str]]:
        """
        –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo Instant Answer API
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        try:
            session = await self._get_session()
            
            params = {
                'q': query,
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫ DuckDuckGo: '{query}'")
            
            async with session.get(
                self.search_api,
                params=params,
                timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                # DuckDuckGo –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 202 (Accepted) - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                if response.status == 202:
                    logger.warning(f"DuckDuckGo –≤–µ—Ä–Ω—É–ª 202 (–∑–∞–ø—Ä–æ—Å –ø—Ä–∏–Ω—è—Ç, –Ω–æ –Ω–µ—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)")
                    return []
                
                if response.status != 200:
                    logger.error(f"DuckDuckGo API error: {response.status}")
                    return []
                
                data = await response.json()
                results = []
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                if data.get('AbstractText'):
                    results.append({
                        'title': data.get('Heading', 'DuckDuckGo Answer'),
                        'snippet': data['AbstractText'],
                        'url': data.get('AbstractURL', ''),
                        'source': data.get('AbstractSource', 'DuckDuckGo')
                    })
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ç–µ–º
                for topic in data.get('RelatedTopics', [])[:max_results - len(results)]:
                    if isinstance(topic, dict) and 'Text' in topic:
                        results.append({
                            'title': topic.get('FirstURL', '').split('/')[-1].replace('_', ' '),
                            'snippet': topic.get('Text', ''),
                            'url': topic.get('FirstURL', ''),
                            'source': 'DuckDuckGo'
                        })
                
                logger.info(f"DuckDuckGo –≤–µ—Ä–Ω—É–ª {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return results[:max_results]
                
        except asyncio.TimeoutError:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ DuckDuckGo")
            return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ DuckDuckGo: {e}", exc_info=True)
            return []
    
    async def search_duckduckgo_html(
        self,
        query: str,
        max_results: int = 5
    ) -> List[Dict[str, str]]:
        """
        –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ HTML –≤–µ—Ä—Å–∏—é DuckDuckGo
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ Instant Answer API –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        try:
            session = await self._get_session()
            
            params = {
                'q': query,
                'kl': 'ru-ru'  # –†–µ–≥–∏–æ–Ω
            }
            
            logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è HTML –ø–æ–∏—Å–∫ DuckDuckGo: '{query}'")
            
            async with session.get(
                "https://html.duckduckgo.com/html/",
                params=params,
                timeout=aiohttp.ClientTimeout(total=15),
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            ) as response:
                if response.status != 200:
                    logger.error(f"DuckDuckGo HTML error: {response.status}")
                    return []
                
                html = await response.text()
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å BeautifulSoup –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    results = []
                    
                    # –ò—â–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
                    result_divs = soup.find_all('div', class_='result')
                    
                    for div in result_divs[:max_results]:
                        try:
                            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
                            title_link = div.find('a', class_='result__a')
                            if not title_link:
                                continue
                            
                            title = title_link.get_text(strip=True)
                            href = title_link.get('href', '')
                            
                            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π URL (DuckDuckGo –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç—ã)
                            if href.startswith('//duckduckgo.com/l/?'):
                                import urllib.parse
                                parsed = urllib.parse.urlparse(href)
                                params = urllib.parse.parse_qs(parsed.query)
                                url = params.get('uddg', [''])[0]
                                if not url:
                                    continue
                                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º URL
                                url = urllib.parse.unquote(url)
                            else:
                                url = href
                            
                            # –û–ø–∏—Å–∞–Ω–∏–µ (snippet)
                            snippet_elem = div.find('a', class_='result__snippet')
                            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                            
                            if not snippet:
                                snippet = f'–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è "{query}"'
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                            try:
                                from urllib.parse import urlparse
                                domain = urlparse(url).netloc
                                source = domain.replace('www.', '')
                            except:
                                source = 'Web'
                            
                            results.append({
                                'title': title,
                                'snippet': snippet[:300],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                                'url': url,
                                'source': source
                            })
                            
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                            continue
                    
                    logger.info(f"DuckDuckGo HTML –≤–µ—Ä–Ω—É–ª {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤")
                    return results
                    
                except ImportError:
                    # Fallback –Ω–∞ regex –µ—Å–ª–∏ BeautifulSoup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
                    logger.warning("BeautifulSoup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥")
                    import re
                    results = []
                    
                    pattern = r'<a class="result__a"[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
                    matches = re.findall(pattern, html)
                    
                    for url, title in matches[:max_results]:
                        if url and title:
                            if url.startswith('//duckduckgo.com/l/?'):
                                continue
                            
                            results.append({
                                'title': title.strip(),
                                'snippet': f'–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è "{query}"',
                                'url': url,
                                'source': 'Web'
                            })
                    
                    logger.info(f"DuckDuckGo HTML –≤–µ—Ä–Ω—É–ª {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    return results
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ HTML –ø–æ–∏—Å–∫–∞ DuckDuckGo: {e}", exc_info=True)
            return []
    
    async def search_wikipedia(self, query: str, lang: str = 'en') -> Optional[Dict[str, str]]:
        """
        –ü–æ–∏—Å–∫ –≤ Wikipedia —á–µ—Ä–µ–∑ API
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            lang: –Ø–∑—ã–∫ Wikipedia (en, ru, etc.)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –∏–∑ Wikipedia –∏–ª–∏ None
        """
        try:
            session = await self._get_session()
            
            logger.info(f"–ü–æ–∏—Å–∫ –≤ Wikipedia ({lang}): '{query}'")
            
            # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏
            search_url = f"https://{lang}.wikipedia.org/w/api.php"
            search_params = {
                'action': 'query',
                'format': 'json',
                'list': 'search',
                'srsearch': query,
                'srlimit': 1
            }
            
            async with session.get(
                search_url,
                params=search_params,
                timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status != 200:
                    return None
                
                search_data = await response.json()
                search_results = search_data.get('query', {}).get('search', [])
                
                if not search_results:
                    logger.info("Wikipedia: –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    return None
                
                page_id = search_results[0]['pageid']
                title = search_results[0]['title']
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            content_params = {
                'action': 'query',
                'format': 'json',
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'pageids': page_id
            }
            
            async with session.get(
                search_url,
                params=content_params,
                timeout=aiohttp.ClientTimeout(total=15)
            ) as response:
                if response.status != 200:
                    return None
                
                content_data = await response.json()
                pages = content_data.get('query', {}).get('pages', {})
                
                if str(page_id) in pages:
                    extract = pages[str(page_id)].get('extract', '')
                    
                    logger.info(f"Wikipedia: –Ω–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è '{title}'")
                    
                    return {
                        'title': title,
                        'snippet': extract[:500] + '...' if len(extract) > 500 else extract,
                        'url': f"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}",
                        'source': f'Wikipedia ({lang.upper()})'
                    }
                
                return None
                
        except asyncio.TimeoutError:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ Wikipedia")
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Wikipedia: {e}", exc_info=True)
            return None
    
    async def get_current_time_info(self) -> Dict[str, str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏
        
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–º –≤—Ä–µ–º–µ–Ω–∏
        """
        now = datetime.now()
        
        return {
            'title': '–¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è',
            'snippet': (
                f"–î–∞—Ç–∞: {now.strftime('%d.%m.%Y')}\n"
                f"–í—Ä–µ–º—è: {now.strftime('%H:%M:%S')}\n"
                f"–î–µ–Ω—å –Ω–µ–¥–µ–ª–∏: {now.strftime('%A')}"
            ),
            'url': '',
            'source': 'System'
        }
    
    async def fetch_url_content(
        self,
        url: str,
        max_length: int = 2000
    ) -> Optional[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ URL —Å —É–º–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–ª–∏ None
        """
        try:
            session = await self._get_session()
            
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ URL: {url}")
            
            async with session.get(
                url,
                timeout=aiohttp.ClientTimeout(total=20),
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            ) as response:
                if response.status != 200:
                    logger.warning(f"URL –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                    return None
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
                html = await response.text()
                
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BeautifulSoup –¥–ª—è —É–º–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                    for script in soup(['script', 'style', 'nav', 'footer', 'header']):
                        script.decompose()
                    
                    # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                    main_content = soup.find('article') or soup.find('main') or soup.find('body')
                    
                    if main_content:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                        paragraphs = main_content.find_all(['p', 'h1', 'h2', 'h3', 'li'])
                        text_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
                        clean_text = '\n'.join(text_parts)
                    else:
                        clean_text = soup.get_text(separator='\n', strip=True)
                    
                    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
                    import re
                    clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text)
                    clean_text = re.sub(r' +', ' ', clean_text)
                    
                    logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(clean_text)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
                    return clean_text[:max_length]
                    
                except ImportError:
                    # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é –æ—á–∏—Å—Ç–∫—É HTML
                    logger.warning("BeautifulSoup –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –æ—á–∏—Å—Ç–∫—É")
                    import re
                    clean_text = re.sub(r'<[^>]+>', '', html)
                    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
                    return clean_text[:max_length]
                
        except asyncio.TimeoutError:
            logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ URL")
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ URL: {e}")
            return None
    
    async def search(
        self,
        query: str,
        max_results: int = 5,
        include_wikipedia: bool = True
    ) -> List[Dict[str, str]]:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            include_wikipedia: –í–∫–ª—é—á–∏—Ç—å –ø–æ–∏—Å–∫ –≤ Wikipedia
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        all_results = []
        
        # –ü–æ–∏—Å–∫ –≤ DuckDuckGo (Instant Answer API)
        ddg_results = await self.search_duckduckgo(query, max_results)
        all_results.extend(ddg_results)
        
        # –ï—Å–ª–∏ DuckDuckGo API –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 202), –ø—Ä–æ–±—É–µ–º HTML –≤–µ—Ä—Å–∏—é
        if len(all_results) == 0:
            logger.info("API –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º HTML –≤–µ—Ä—Å–∏—é DuckDuckGo")
            ddg_html_results = await self.search_duckduckgo_html(query, max_results)
            all_results.extend(ddg_html_results)
        
        # –ï—Å–ª–∏ DuckDuckGo –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–ª–∏ –¥–∞–ª –º–∞–ª–æ, –ø—Ä–æ–±—É–µ–º Wikipedia
        if include_wikipedia and len(all_results) < max_results:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
            is_russian = any(ord('–∞') <= ord(char.lower()) <= ord('—è') for char in query)
            
            if is_russian:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫—É—é Wikipedia
                logger.info("–ó–∞–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –ø—Ä–æ–±—É–µ–º ru.wikipedia.org")
                wiki_result = await self.search_wikipedia(query, lang='ru')
                if wiki_result:
                    all_results.append(wiki_result)
                
                # –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë –º–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é
                if len(all_results) < max_results:
                    wiki_result_en = await self.search_wikipedia(query, lang='en')
                    if wiki_result_en:
                        all_results.append(wiki_result_en)
            else:
                # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ - —Ç–æ–ª—å–∫–æ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è Wikipedia
                wiki_result = await self.search_wikipedia(query, lang='en')
                if wiki_result:
                    all_results.append(wiki_result)
        
        logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_results)}")
        return all_results[:max_results]
    
    async def search_with_content(
        self,
        query: str,
        max_results: int = 3,
        fetch_content: bool = True
    ) -> List[Dict[str, str]]:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        –î–∞—ë—Ç –±–æ–ª–µ–µ –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è LLM
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            fetch_content: –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        # –°–Ω–∞—á–∞–ª–∞ –¥–µ–ª–∞–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
        results = await self.search(query, max_results=max_results)
        
        if not fetch_content or not results:
            return results
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        enhanced_results = []
        for result in results:
            url = result.get('url', '')
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –Ω–µ—Ç URL –∏–ª–∏ —ç—Ç–æ Wikipedia (—É–∂–µ –µ—Å—Ç—å snippet)
            if not url or 'wikipedia.org' in url:
                enhanced_results.append(result)
                continue
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content = await self.fetch_url_content(url, max_length=1500)
            
            if content:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
                result['full_content'] = content
                result['snippet'] = content[:500] + '...' if len(content) > 500 else content
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç —Å {result.get('source', '—Å–∞–π—Ç–∞')}")
            
            enhanced_results.append(result)
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(enhanced_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º")
        return enhanced_results
    
    def format_search_results(
        self,
        results: List[Dict[str, str]],
        query: str
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ LLM
        
        Args:
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        if not results:
            return f"–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        sources_count = {}
        for result in results:
            source = result.get('source', 'Unknown')
            sources_count[source] = sources_count.get(source, 0) + 1
        
        formatted = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n"
        formatted += f"–ù–∞–π–¥–µ–Ω–æ: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å {len(sources_count)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n\n"
        
        for i, result in enumerate(results, 1):
            title = result.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            source = result.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
            url = result.get('url', '')
            snippet = result.get('snippet', '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è')
            
            formatted += f"‚ïê‚ïê‚ïê –†–µ–∑—É–ª—å—Ç–∞—Ç #{i} ‚ïê‚ïê‚ïê\n"
            formatted += f"üìå {title}\n"
            formatted += f"üåê –ò—Å—Ç–æ—á–Ω–∏–∫: {source}\n"
            
            if url:
                formatted += f"üîó URL: {url}\n"
            
            formatted += f"üìÑ –û–ø–∏—Å–∞–Ω–∏–µ:\n{snippet}\n"
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
            if 'full_content' in result and result['full_content']:
                formatted += f"\nüìñ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç (—Ñ—Ä–∞–≥–º–µ–Ω—Ç):\n{result['full_content'][:800]}...\n"
            
            formatted += "\n"
        
        formatted += f"üí° –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —ç—Ç–∏—Ö {len(results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.\n"
        
        return formatted


class SearchEnhancedLLM:
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è LLM —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–µ–±-–ø–æ–∏—Å–∫–∞
    –ü–æ–∑–≤–æ–ª—è–µ—Ç LLM –ø–æ–ª—É—á–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
    """
    
    def __init__(self, lm_client, web_search_tool: WebSearchTool):
        self.lm_client = lm_client
        self.web_search = web_search_tool
    
    async def generate_with_search(
        self,
        user_message: str,
        conversation_history: List[Dict] = None,
        system_prompt: str = None,
        auto_search: bool = True
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤–µ–±-–ø–æ–∏—Å–∫–æ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        
        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            auto_search: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞
            
        Returns:
            –û—Ç–≤–µ—Ç LLM —Å —É—á–µ—Ç–æ–º –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫
        search_keywords = [
            # –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
            '—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫—Ç–æ —Ç–∞–∫–æ–π', '–∫—Ç–æ —Ç–∞–∫–∞—è', '–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–∫–æ–≥–¥–∞',
            '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '—Å–∫–æ–ª—å–∫–æ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º',
            
            # –ó–∞–ø—Ä–æ—Å—ã –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            '–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏', '–∞–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–Ω–æ–≤–æ—Å—Ç–∏',
            '–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è', '—á—Ç–æ –Ω–æ–≤–æ–≥–æ', '—Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏',
            '—Å–µ–≥–æ–¥–Ω—è', '–≤—á–µ—Ä–∞', '–Ω–µ–¥–∞–≤–Ω–æ', '–≤ —ç—Ç–æ–º –≥–æ–¥—É', '–≤ —ç—Ç–æ–º –º–µ—Å—è—Ü–µ',
            
            # –Ø–≤–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–∏—Å–∫–∞
            '–ø–æ–∏—Å–∫', '–Ω–∞–π–¥–∏', '–Ω–∞–π–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é', '—Ä–∞—Å—Å–∫–∞–∂–∏ –æ', 
            '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ', '—É–∑–Ω–∞–π', '–ø—Ä–æ–≤–µ—Ä—å', '–ø–æ–≥—É–≥–ª–∏',
            
            # –í–æ–ø—Ä–æ—Å—ã –æ —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
            '–∫—Ç–æ —Å–µ–π—á–∞—Å', '–∫—Ç–æ —è–≤–ª—è–µ—Ç—Å—è', '–∫—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç', '–∫—Ç–æ –≤–æ–∑–≥–ª–∞–≤–ª—è–µ—Ç',
            '—Ç–µ–∫—É—â–∏–π', '—Å–µ–π—á–∞—Å', '–Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç', '–≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è',
            '–∞–∫—Ç—É–∞–ª—å–Ω—ã–π', '—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π',
            
            # –í–æ–ø—Ä–æ—Å—ã —Ç—Ä–µ–±—É—é—â–∏–µ —Ñ–∞–∫—Ç–æ–≤
            '—Ñ–∞–∫—Ç—ã –æ', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', '–¥–∞–Ω–Ω—ã–µ –æ', '—Ü–∏—Ñ—Ä—ã',
            '–ø–æ–±–µ–¥–∏—Ç–µ–ª—å', '–ª–∏–¥–µ—Ä', '—á–µ–º–ø–∏–æ–Ω', '—Ä–µ–∫–æ—Ä–¥',
            
            # –°–æ–±—ã—Ç–∏—è –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∏
            '–±–∏–æ–≥—Ä–∞—Ñ–∏—è', '–∏—Å—Ç–æ—Ä–∏—è', '–¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è', '–∫–∞—Ä—å–µ—Ä–∞'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
        english_keywords = [
            'what is', 'who is', 'where is', 'when', 'how',
            'latest news', 'current', 'today', 'recent',
            'search for', 'find', 'tell me about', 'information about',
            'who won', 'winner', 'champion', 'latest'
        ]
        
        message_lower = user_message.lower()
        
        needs_search = auto_search and (
            any(keyword in message_lower for keyword in search_keywords) or
            any(keyword in message_lower for keyword in english_keywords)
        )
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫ –∏ –æ–Ω–æ –∫–æ—Ä–æ—Ç–∫–æ–µ
        # (–≤–µ—Ä–æ—è—Ç–Ω–æ, –ø—Ä–æ—Å—Ç–æ–π –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É—é—â–∏–π —Ñ–∞–∫—Ç–∞)
        if not needs_search and '?' in user_message and len(user_message.split()) < 15:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            first_words = message_lower.split()[:2]
            question_words = ['–∫—Ç–æ', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–∞–∫', '–∫–∞–∫–æ–π', 
                            'who', 'what', 'where', 'when', 'why', 'how', 'which']
            if any(word in question_words for word in first_words):
                needs_search = True
        
        if needs_search:
            logger.info(f"üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {user_message[:50]}...")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            search_results = await self.web_search.search_with_content(
                user_message, 
                max_results=3,
                fetch_content=True  # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü
            )
            
            if search_results:
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                search_context = self.web_search.format_search_results(
                    search_results,
                    user_message
                )
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫ —Å–æ–æ–±—â–µ–Ω–∏—é
                enhanced_message = (
                    f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_message}\n\n"
                    f"–ê–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:\n{search_context}\n\n"
                    f"–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞. "
                    f"–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–ª–µ–∑–Ω–∞, —É–ø–æ–º—è–Ω–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞. "
                    f"–û—Ç–≤–µ—á–∞–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –Ω–µ —É–ø–æ–º–∏–Ω–∞—è, —á—Ç–æ —Ç—ã –∏—Å–∫–∞–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, "
                    f"–ø—Ä–æ—Å—Ç–æ –¥–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
                )
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                return await self.lm_client.generate_response(
                    user_message=enhanced_message,
                    conversation_history=conversation_history,
                    system_prompt=system_prompt
                )
            else:
                logger.info("‚ùå –ü–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é")
        
        # –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ –ø–æ–∏—Å–∫–∞
        return await self.lm_client.generate_response(
            user_message=user_message,
            conversation_history=conversation_history,
            system_prompt=system_prompt
        )
    
    async def search_and_summarize(self, query: str) -> str:
        """
        –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        logger.info(f"üîç –ü–æ–∏—Å–∫ –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: '{query}'")
        
        search_results = await self.web_search.search(query, max_results=5)
        
        if not search_results:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        context = self.web_search.format_search_results(search_results, query)
        
        # –ü—Ä–æ—Å–∏–º LLM —Å–æ–∑–¥–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        summary_prompt = (
            f"–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ "
            f"—Ä–µ–∑—é–º–µ:\n\n{context}\n\n"
            f"–£–ø–æ–º—è–Ω–∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        )
        
        return await self.lm_client.generate_response(
            user_message=summary_prompt,
            system_prompt="–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."
        )
